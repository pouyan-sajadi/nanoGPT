# End-to-End GPT-2 Fine-Tuning and Cloud Deployment with Multi-GPU Scaling

## Overview

This project builds on top of [nanoGPT](https://github.com/karpathy/nanoGPT) by Andrej Karpathy, a minimal, fast, and simple repository for training and fine-tuning GPT models. The primary objective is to fine-tune OpenAI's GPT-2 model on a custom dataset and compare the results with the original GPT-2 model. Additionally, the project implements the model training and inference pipeline on a cloud environment, leveraging Compute Canada's multi-GPU infrastructure.

## Objectives

1. **Fine-Tune GPT-2 Model**
   - Fine-tune the GPT-2 model (released by OpenAI in 2019) on a selected dataset.
   - Compare the performance of the fine-tuned model with the original GPT-2 using relevant evaluation metrics.
   
2. **Cloud Implementation**
   - Implement the model training and inference pipeline on Compute Canada.
   - Leverage Compute Canada's multi-GPU environment to optimize the training process.
